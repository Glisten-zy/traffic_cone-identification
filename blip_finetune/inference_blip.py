from transformers import BlipProcessor, BlipForConditionalGeneration
from PIL import Image
import torch

# === ä¿®æ”¹ä¸ºä½ æ¨¡å‹ä¿å­˜è·¯å¾„ ===
model_path = r"E:\rubbercone-captioning\blip_finetuned\checkpoint-130"
image_path = r"E:\rubbercone-captioning\images\01cd713f4d105844b04b3dd3577e76df.jpg"  # æ›¿æ¢ä¸ºä½ è¦é¢„æµ‹çš„å›¾åƒè·¯å¾„

# === åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ ===
processor = BlipProcessor.from_pretrained(model_path)
model = BlipForConditionalGeneration.from_pretrained(model_path)
model.eval()

# === åŠ è½½å›¾ç‰‡ ===
image = Image.open(image_path).convert("RGB")
inputs = processor(images=image, return_tensors="pt")

# === æ¨ç† ===
with torch.no_grad():
    out = model.generate(**inputs, max_new_tokens=50)

# === è¾“å‡ºæè¿° ===
caption = processor.decode(out[0], skip_special_tokens=True)
print("ğŸ“ å›¾åƒæè¿°ï¼š", caption)
